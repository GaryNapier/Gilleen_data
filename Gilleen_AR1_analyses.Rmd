---
title: "AR1 analyses of Gilleen data"
author: "Gary Napier"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: html_document
---

<!-- arguments -->

```{r echo = FALSE, cache=TRUE}
Subjects <- 1
Analyses_nos <- c(1)
Do_stats = TRUE
Do_plots = TRUE

```

<!-- Custom functions, load data & clean -->

```{r echo = FALSE, cache=TRUE}

tryCatch({
setwd("U:\\Gilleen_data")
}, error=function(e){
setwd("~/Dropbox/Peters_data")
})

# Source packages and custom function
source("https://raw.githubusercontent.com/GaryNapier/Packages_functions/master/PACKAGES.R")

#---------------------------------------
# Load in data & clean
#---------------------------------------

# Read in data from CSV and Matlab files
# Gillen CSV file is for getting the groups (control/patient/normal control)
if (!exists("Gilleen") | !exists("Gilleen_analyses")){
  Gilleen_PDG_data <<- readMat("Gilleen_PDG_data_flip.mat")
  Gilleen_beads_data <<- readMat("Gilleen_Beads_data_flip.mat")
  Gilleen_analyses_results <<- readMat("Gilleen_analysis_results.mat")
}

# # Get groups and clean data
# Group <- Gilleen$cond
# Group <- Group[complete.cases(Group)]
# levels(Group)[levels(Group) == "normal control"] <- "Control"
# levels(Group)[levels(Group) == "deluded"] <- "Patient"
# Group <- factor(Group, levels = c("Control", "Patient"))
# Group_names <- as.character(unique(Group))

#---------------------------------------
# Misc setup
#---------------------------------------

# Groups
Ctrl <- 1:112
Pat <- 113:168
Group <- factor(c(rep("Control", max(Ctrl)), rep("Patient", max(Pat)-min(Pat)+1)), 
                 levels = c("Control", "Patient"))
Group_names <- as.character(unique(Group))

# For tables
Blue <- "#F1F0FA"
Table_options <- list(align=c("c","|"), align.header =c("c","|"), col.columns = c("none",Blue) )

# Get model names/numbers
Model_names <- vector()
for (i in seq(Analyses_nos)){
  Model_names[i] <- sprintf("A%g", Analyses_nos[i])  
}

# Save names of priors
Priors <- c("NuPrimeMu", "NuPrimeSa", "KappaMu", "KappaSa", "OmegaMu", 
            "OmegaSa", "ThetaMu", "ThetaSa", "mMu", "mSa", "PhiMu", "PhiSa", "Sig2Mu", "Sig2Sa")

Priors_names <- c("NuPrimeMu", "NuPrimeSa", "KappaMu", "KappaSa", "Om2Mu", 
            "Om2Sa", "Om3Mu", "Om3Sa",  "ThetaMu", "ThetaSa", "m2Mu", "m2Sa", 
            "m3Mu", "m3Sa", "Phi2Mu", "Phi2Sa", "Phi3Mu", "Phi3Sa", "Sig2Mu", "Sig2Sa")
  
# Misc
options(digits = 4)
N_groups <- length(Group_names)
Parameter_names <- c("Om2", "Om3", "log(Nu)", "M2", "M3", "Phi2", "Phi3")
N_parameters <- length(Parameter_names)
N_subjects <- length(Gilleen_analyses_results[[1]][[1]][[1]][[1]])
N_analyses <- length(Analyses_nos)
Dataset_used <- Gilleen_analyses_results[[1]][[1]][[1]][,,1][3]

#----------------------------------------------------------------
# Get & display parameter priors used for each selected analysis
#----------------------------------------------------------------

# Create data frames of priors for selected analyses 
# Separate data frames for original and AR1 models
Priors_df <- data.frame()
Priors_df_AR1 <- list()
for (i in seq(Analyses_nos)){
   Priors_df_AR1[[i]] <- Gilleen_analyses_results[[1]][[Analyses_nos[i]]][[1]][,,1][c(Priors)]
}

# AR1 priors - horrible data-cleaning excercise!
# AR1 priors - horrible data-cleaning excercise!
Priors_df_AR1 <- Priors_df_AR1[!sapply(Priors_df_AR1, is.null)]
Priors_df_AR1 <- lapply(Priors_df_AR1, function(x) x[!sapply(x, is.null)])
Priors_df_AR1 <- lapply(Priors_df_AR1, function(x) lapply(x, function(x){
  paste(as.character(signif(x, digits = 3)), collapse = ", ")
}))
Priors_df_AR1 <- plyr::ldply(lapply(Priors_df_AR1, data.frame), data.frame)
rownames(Priors_df_AR1) <- Model_names

# # Add analyses numbers as row names for each dataframe
# for (i in seq(Analyses_nos)){
#   if (Analyses_nos[i] <= 10){
#    rownames(Priors_df)[i] <- Model_names[i]
#   }else if (Analyses_nos[i] > 10 & length(Priors_df_AR1) > 1){
#    rownames(Priors_df_AR1)[i - nrow(Priors_df)] <- Model_names[i]
#   }
# }

# Tests for R markdown evaluation:
# Priors_df_test <- nrow(Priors_df) > 1
# Priors_df_AR1_test <- nrow(Priors_df_AR1) > 1
  
```
### **Parameter priors**
#### AR1 model analyses:
`r  do.call("htmlTable", c(list(Priors_df_AR1), Table_options))`
**-------------------------------------------------------------------------------------------**

<!-- GET POSTERIORS AND STATS TO PASS TO PLOTS/ DISPLAYS -->

```{r echo=FALSE, cache=TRUE}

# Get posteriors from selected analyses - for overall stats
Om2_vector <- vector("list", length(Gilleen_analyses_results[[1]]))
Om3_vector <- vector("list", length(Gilleen_analyses_results[[1]]))
Nu_vector <- vector("list", length(Gilleen_analyses_results[[1]]))
M2_vector  <- vector("list", length(Gilleen_analyses_results[[1]]))
M3_vector  <- vector("list", length(Gilleen_analyses_results[[1]]))
Phi2_vector<- vector("list", length(Gilleen_analyses_results[[1]]))
Phi3_vector<- vector("list", length(Gilleen_analyses_results[[1]]))
for (j in seq_along(Analyses_nos)){
  for (i in 1:N_subjects){
Om2_vector[[Analyses_nos[j]]][i] <- Gilleen_analyses_results[[1]][[Analyses_nos[j]]][[1]][[1]][[i]][[1]][9][[1]][[6]][2]
Om3_vector[[Analyses_nos[j]]][i] <- Gilleen_analyses_results[[1]][[Analyses_nos[j]]][[1]][[1]][[i]][[1]][9][[1]][[6]][3]
Nu_vector[[Analyses_nos[j]]][i] <- Gilleen_analyses_results[[1]][[Analyses_nos[j]]][[1]][[1]][[i]][[1]][[10]][[1]][1]
M2_vector[[Analyses_nos[j]]][i] <- Gilleen_analyses_results[[1]][[Analyses_nos[j]]][[1]][[1]][[i]][[1]][9][[1]][[4]][2]
M3_vector[[Analyses_nos[j]]][i] <- Gilleen_analyses_results[[1]][[Analyses_nos[j]]][[1]][[1]][[i]][[1]][9][[1]][[4]][3]
Phi2_vector[[Analyses_nos[j]]][i] <- Gilleen_analyses_results[[1]][[Analyses_nos[j]]][[1]][[1]][[i]][[1]][9][[1]][[3]][2]
Phi3_vector[[Analyses_nos[j]]][i] <- Gilleen_analyses_results[[1]][[Analyses_nos[j]]][[1]][[1]][[i]][[1]][9][[1]][[3]][3]
  }#for (i in 1:N_subjects)
}#for (j in Analyses_nos)
 
# Clear NULL values from lists
Om2_vector <- Om2_vector[vapply(Om2_vector, Negate(is.null), NA)]
Om3_vector <- Om3_vector[vapply(Om3_vector, Negate(is.null), NA)]
Nu_vector <- Nu_vector[vapply(Nu_vector, Negate(is.null), NA)]
M2_vector  <- M2_vector[vapply(M2_vector, Negate(is.null), NA)]
M3_vector  <- M3_vector[vapply(M3_vector, Negate(is.null), NA)]
Phi2_vector<- Phi2_vector[vapply(Phi2_vector, Negate(is.null), NA)]
Phi3_vector<- Phi3_vector[vapply(Phi3_vector, Negate(is.null), NA)]

# Transform Nu
Nu_vector <- lapply(Nu_vector, log)

# Make list of data frames for ANOVA analyses
Om2_vector <- lapply(Om2_vector, function(Om2_vector) cbind.data.frame(Group, Om2_vector))
Om3_vector <- lapply(Om3_vector, function(Om3_vector) cbind.data.frame(Group, Om3_vector))
Nu_vector <- lapply(Nu_vector, function(Nu_vector) cbind.data.frame(Group, Nu_vector))
M2_vector  <- lapply(M2_vector, function(M2_vector) cbind.data.frame(Group, M2_vector))
M3_vector  <- lapply(M3_vector, function(M3_vector) cbind.data.frame(Group, M3_vector))
Phi2_vector<- lapply(Phi2_vector, function(Phi2_vector) cbind.data.frame(Group, Phi2_vector))
Phi3_vector<- lapply(Phi3_vector, function(Phi3_vector) cbind.data.frame(Group, Phi3_vector))
  
# Measure fit for whole analyses selected

# Get responses (i.e. data), bead seq & HGF predictions
Responses <- vector("list", length(Gilleen_analyses_results[[1]]))
Beads <- vector("list", length(Gilleen_analyses_results[[1]]))
Predictions <- vector("list", length(Gilleen_analyses_results[[1]]))
Mu2_0 <- vector("list", length(Gilleen_analyses_results[[1]]))
Learning_rate <- vector("list", length(Gilleen_analyses_results[[1]]))
for (j in seq_along(Analyses_nos)){
  for (i in 1:N_subjects){
# r.y:
Responses[[Analyses_nos[j]]][[i]] <- Gilleen_analyses_results[[1]][[Analyses_nos[j]]][[1]][[1]][[i]][[1]][[1]]
# r.u:
Beads[[Analyses_nos[j]]][[i]] <- Gilleen_analyses_results[[1]][[Analyses_nos[j]]][[1]][[1]][[i]][[1]][[2]]
  # r.traj.mu:
  Predictions[[Analyses_nos[j]]][[i]] <- Tapas_sgm(Gilleen_analyses_results[[1]][[Analyses_nos[j]]][[1]][[1]][[i]][[1]][[11]][[1]][,2], 1) 
  # r.p_prc.mu2_0; the '0th' prediction (prior):
  Mu2_0[[Analyses_nos[j]]][[i]] <- Tapas_sgm(Gilleen_analyses_results[[1]][[Analyses_nos[j]]][[1]][[1]][[i]][[1]][[9]][[1]][2], 1)
  # r.traj.wt; Learning rate
  Learning_rate[[Analyses_nos[j]]][[i]] <- Gilleen_analyses_results[[1]][[Analyses_nos[j]]][[1]][[1]][[i]][[1]][[11]][[11]][,1]
  }#(i in 1:N_subjects)
}#(j in 1:length(Analyses_nos))

# Remove NULLS. Not sure why each one is different...
Responses <- Responses[vapply(Responses, Negate(is.null), NA)]
Beads <- Beads[vapply(Beads, Negate(is.null), NA)]
Predictions <- Predictions[vapply(Predictions, Negate(is.null), NA)]
Mu2_0 <- Filter(Negate(is.null), Mu2_0)
Learning_rate <- Learning_rate[vapply(Learning_rate, Negate(is.null), NA)]
# DONE

# Useful!
N_beads <- lapply(Beads, function(x) unlist(unique(lapply(x, length))))
N_predictions <- lapply(Predictions, function(x) unlist(unique(lapply(x, length))))
N_responses <- lapply(Responses, function(x) unlist(unique(lapply(x, length))))

# Pre-allocate for for-loops
Opp_bead_indeces <- vector("list", length(Beads)) 
Opp_bead_indeces <- lapply(Opp_bead_indeces, function(x){
  vector("list", N_subjects)
})

# Loops for getting true/false for 'bead of opposite colour' from bead seqs
for (k in 1:length(Beads)){
  for (j in 1:length(Beads[[k]])){
    for(i in 2:length(Beads[[k]][[j]])){
      Opp_bead_indeces[[k]][[j]][i] <- !(Beads[[k]][[j]][i]-Beads[[k]][[j]][i-1] == 0)
    }
  }
}
# Find indeces
Opp_bead_indeces <- lapply(Opp_bead_indeces, function(x){
  lapply(x, function(y) which(y == TRUE))
})
Fit_BOOC_whole_model <- vector("list", length(Analyses_nos))

for (i in 1:length(Opp_bead_indeces)){
  for (j in 1:length(Opp_bead_indeces[[i]])){
    
    Fit_BOOC_whole_model[[i]][j] <- 
      signif(sum((Responses[[i]][[j]][Opp_bead_indeces[[i]][[j]]] 
                  - Predictions[[i]][[j]][Opp_bead_indeces[[i]][[j]]])^2), digits = 2)
  }
}
  
# Make a copy to pass to 'individual subjects' section - allows simple 
# subsetting of BOOC fit by subjects selected
Fit_BOOC_whole_model_copy <- Fit_BOOC_whole_model
# Weight Fit_BOOC_whole_model_copy by length of trials (number of beads) divided by 10 
Fit_BOOC_subject <- lapply(seq_along(Fit_BOOC_whole_model_copy), function(i){
  (Fit_BOOC_whole_model_copy[[i]]/(N_beads[[i]]/10))
})

# Take the mean of the fits and adjust by length of bead seq (N_beads) for each analysis
Fit_BOOC_whole_model <- lapply_mod(Fit_BOOC_whole_model,
                                   function(i) mean(Fit_BOOC_whole_model[[i]])/N_beads[[i]])

# For predictions, need to concatenate Mu2_0 prior at the start of each Prediction vector
for (j in seq(Predictions))
  for (i in seq(Predictions[[j]])){
    Predictions[[j]][[i]] <- append(Predictions[[j]][[i]], Mu2_0[[j]][i], 0)
  }
```
  

<!--  CREATE parametric models - t-tests -->
```{r echo=FALSE, cache=F}
#------------------------------------------------
# CREATE parametric models - t-tests
#------------------------------------------------
# Create t-tests:
# Om2
T_om2 <- lapply(Om2_vector, function(x) t.test(x$Om2_vector ~ x$Group))
lapply(Om2_vector, function(x) cohensD(Om2_vector ~ Group, data = x))

# Om3
T_om3 <- lapply(Om3_vector, function(x) t.test(x$Om3_vector ~ x$Group))

# Nu model
T_nu <- lapply(Nu_vector, function(x) t.test(x$Nu_vector ~ x$Group))

# M2
T_M2 <- lapply(M2_vector, function(x) t.test(x$M2_vector ~ x$Group))

# M3
T_M3 <- lapply(M3_vector, function(x) t.test(x$M3_vector ~ x$Group))

# Phi2
T_phi2 <- lapply(Phi2_vector, function(x) t.test(x$Phi2_vector ~ x$Group))

# Phi3
T_phi3 <- lapply(Phi3_vector, function(x) t.test(x$Phi3_vector ~ x$Group))

# Prep post-hoc comparisons for plot as data frame

Labels <- factor(c(rep("Omega2", 1), rep("Omega3", 1),  rep("Nu", 1), rep("M2", 1), rep("M3", 1), rep("Phi2", 1), rep("Phi3", 1)), levels = c("Omega2", "Omega3", "Nu", "M2", "M3", "Phi2", "Phi3")) 

# Data frame of t-tests
T_tests <- c(T_om2, T_om3, T_nu, T_M2, T_M3, T_phi2, T_phi3)

T_tests <- data.frame(sapply(T_tests, function(x) x[c("statistic", "parameter", "p.value")]))

colnames(T_tests) <- Parameter_names


# Effect size cohen's d
cohensD()


```

### **Test of ANOVA assumptions**
##### Shapiro-Wilk tests of normality of residuals 
##### [<0.05 = NOT normal]

  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  

